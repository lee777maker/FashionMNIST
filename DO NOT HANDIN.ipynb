{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45bc8de1",
   "metadata": {},
   "source": [
    "# Neural Network Assignment\n",
    "## Report\n",
    "*by Lethabo Neo* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeae87",
   "metadata": {},
   "source": [
    "# Imports, Installations and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d414cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset,random_split\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a28e9",
   "metadata": {},
   "source": [
    "I added random_spilt for random split of train,validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd59b4",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "- Reading files from directory\n",
    "- Converting the images into vectors\n",
    "- Displaying the images\n",
    "- Subsetting the data\n",
    "- Assigning the kind of data to labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6fb65",
   "metadata": {},
   "source": [
    "1. Read files from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "073f053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Users/lethaboneo/Desktop/Computer Science/CSC3022F/Assignments/Machine Learning/ML Assignment 1/\"\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "train = datasets.FashionMNIST(DATA_DIR, train=True, download=False, transform=transform)\n",
    "test = datasets.FashionMNIST(DATA_DIR, train=False, download=False,transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b462e1",
   "metadata": {},
   "source": [
    "*Visualisation purposes*\n",
    "*print the datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fe576",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset:\", train)\n",
    "print(\"Test dataset:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= int(0.9*len(train))\n",
    "val_size= len(train)- train_size\n",
    "trainData, valData = random_split(train,[train_size,val_size])\n",
    "print(\"Training Dataset:\", trainData)\n",
    "print(\"Validation Dataset:\", valData)\n",
    "print(\"Test Dataset:\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248ea66",
   "metadata": {},
   "source": [
    "Split data into Training/Validation/Test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd1fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainData, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valData, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56741ea",
   "metadata": {},
   "source": [
    "Split the data in batches for more efficient and fater training. Process 128 images at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0aacf",
   "metadata": {},
   "source": [
    "# Neural Network implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff27cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size1 , num_classes):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.bn= nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.relu =nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size1)\n",
    "        self.bn1= nn.BatchNorm1d(hidden_size1)\n",
    "        self.dropout1=nn.Dropout(0.5)\n",
    "        self.relu1 =nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size1, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #1 hidden layer\n",
    "        out =self.fc1(x)\n",
    "        out =self.bn(out)\n",
    "        out= self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #2 hidden layer\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14bfeb",
   "metadata": {},
   "source": [
    "# Details\n",
    "My final model is 1 hidden layer Neural Network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0fb39",
   "metadata": {},
   "source": [
    "# Train model\n",
    "*Initilaise Hyperparamters*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afcfadf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NeuralNetwork.__init__() missing 1 required positional argument: 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m val_acc = []\n\u001b[32m     13\u001b[39m best_val_acc = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model = \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     16\u001b[39m optimizer = optim.Adam(model.parameters(), lr=learn_rate,weight_decay=\u001b[32m1e-5\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: NeuralNetwork.__init__() missing 1 required positional argument: 'num_classes'"
     ]
    }
   ],
   "source": [
    "learn_rate = 0.001\n",
    "num_epochs = 30\n",
    "input_size = 28*28\n",
    "hidden_size = 256\n",
    "hidden_size1 = 128\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "best_val_acc = 0\n",
    "model = NeuralNetwork(input_size, hidden_size,num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate,weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',patience=2,factor=0.5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e153ebe",
   "metadata": {},
   "source": [
    "# Validation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0328b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.reshape(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate epoch training loss\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.reshape(-1, input_size)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_epoch += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss_epoch /= len(val_loader)\n",
    "    val_loss.append(val_loss_epoch)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_acc.append(val_accuracy)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_accuracy)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model_nn.pth')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {epoch_loss:.4f}, '\n",
    "          f'Val Loss: {val_loss_epoch:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy:.2f}% '\n",
    "          f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfdf78",
   "metadata": {},
   "source": [
    "# Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd18d197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation:\n",
      "Validation Accuracy: 90.12%\n",
      "Test Accuracy: 89.88%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.reshape(-1, 28*28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "val_accuracy = evaluate_model(model, val_loader)\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38399d7",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test, label='Tr Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "def evaluate_model_comprehensive(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.reshape(-1, 28*28)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Get predictions for validation set\n",
    "val_labels, val_preds = evaluate_model_comprehensive(model, val_loader)\n",
    "\n",
    "# Get predictions for test set\n",
    "test_labels, test_preds = evaluate_model_comprehensive(model, test_loader)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=[\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=[\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "def plot_confusion_matrix(labels, preds, title):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', \n",
    "                xticklabels=[\n",
    "                    'T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boot'\n",
    "                ],\n",
    "                yticklabels=[\n",
    "                    'T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boot'\n",
    "                ])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nValidation Set Confusion Matrix:\")\n",
    "plot_confusion_matrix(val_labels, val_preds, \"Validation Set Confusion Matrix\")\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "plot_confusion_matrix(test_labels, test_preds, \"Test Set Confusion Matrix\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
